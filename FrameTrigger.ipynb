{"cells":[{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-01-06 21:43:08.554622: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2024-01-06 21:43:08.789199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-01-06 21:43:10.209276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["from src.dataio import load_data\n","from pprint import pprint\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","import torch\n","from keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["MAX_LEN = 256"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of instances in trn: 19391\n","# of instances in dev: 2272\n","# of instances in tst: 6714\n","data example: [['Does', 'Iran', '<tgt>', 'have', '</tgt>', 'the', 'infrastructure', 'necessary', 'to', 'produce', 'nuclear', 'weapons', '?'], ['_', '_', '_', 'have.v', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['_', '_', '_', 'Possession', '_', '_', '_', '_', '_', '_', '_', '_', '_'], ['O', 'B-Owner', 'X', 'O', 'X', 'B-Possession', 'I-Possession', 'I-Possession', 'I-Possession', 'I-Possession', 'I-Possession', 'I-Possession', 'O']]\n"]}],"source":["trn, dev, tst = load_data('data/fn1.7_parsed/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def bert_tokenizer(text):\n","    orig_tokens = text.split(' ')\n","    bert_tokens = []\n","    orig_to_tok_map = []\n","    bert_tokens.append(\"[CLS]\")\n","    for orig_token in orig_tokens:\n","        orig_to_tok_map.append(len(bert_tokens))\n","        bert_tokens.extend(self.tokenizer.tokenize(orig_token))\n","    bert_tokens.append(\"[SEP]\")\n","\n","    return orig_tokens, bert_tokens, orig_to_tok_map\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def convert_to_bert_input_JointShallowSemanticParsing( input_data):\n","    tokenized_texts, lus, senses, args = [],[],[],[]\n","    orig_tok_to_maps = []\n","    for i in range(len(input_data)):    \n","        data = input_data[i]\n","        text = ' '.join(data[0])\n","        orig_tokens, bert_tokens, orig_to_tok_map = self.bert_tokenizer(text)\n","\n","        orig_tok_to_maps.append(orig_to_tok_map)\n","        tokenized_texts.append(bert_tokens)\n","\n","        ori_lus = data[1]    \n","        lu_sequence = []\n","        for i in range(len(bert_tokens)):\n","            if i in orig_to_tok_map:\n","                idx = orig_to_tok_map.index(i)\n","                l = ori_lus[idx]\n","                lu_sequence.append(l)\n","            else:\n","                lu_sequence.append('_')\n","        lus.append(lu_sequence)        \n","\n","        if self.mode == 'train':\n","            ori_senses, ori_args = data[2], data[3]\n","            sense_sequence, arg_sequence = [],[]\n","            for i in range(len(bert_tokens)):\n","                if i in orig_to_tok_map:\n","                    idx = orig_to_tok_map.index(i)\n","                    fr = ori_senses[idx]\n","                    sense_sequence.append(fr)\n","                    ar = ori_args[idx]\n","                    arg_sequence.append(ar)\n","                else:\n","                    sense_sequence.append('_')\n","                    arg_sequence.append('X')\n","            senses.append(sense_sequence)\n","            args.append(arg_sequence)\n","\n","    input_ids = pad_sequences([self.tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                            maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    orig_tok_to_maps = pad_sequences(orig_tok_to_maps, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\", value=-1)\n","    \n","    if self.mode =='train':\n","        if self.srl == 'propbank-dp':\n","            arg_ids = pad_sequences([[self.arg2idx.get(ar) for ar in arg] for arg in args],\n","                                    maxlen=MAX_LEN, value=self.arg2idx[\"X\"], padding=\"post\",\n","                                    dtype=\"long\", truncating=\"post\")\n","        elif self.srl == 'framenet-argid':\n","            arg_ids = pad_sequences([[self.bio_argument2idx.get(ar) for ar in arg] for arg in args],\n","                                    maxlen=MAX_LEN, value=self.bio_argument2idx[\"X\"], padding=\"post\",\n","                                    dtype=\"long\", truncating=\"post\")\n","        else:\n","            arg_ids = pad_sequences([[self.bio_arg2idx.get(ar) for ar in arg] for arg in args],\n","                                    maxlen=MAX_LEN, value=self.bio_arg2idx[\"X\"], padding=\"post\",\n","                                    dtype=\"long\", truncating=\"post\")\n","\n","    lu_seq, sense_seq = [],[]\n","    for sent_idx in range(len(lus)):\n","        lu_items = lus[sent_idx]\n","        lu = []\n","        for idx in range(len(lu_items)):\n","            if lu_items[idx] != '_':\n","                if len(lu) == 0:\n","                    if self.mode != 'train' and self.masking == False:\n","                        lu.append(1)\n","                    else:\n","                        lu.append(self.lu2idx[lu_items[idx]])\n","                        \n","        lu_seq.append(lu)\n","        \n","        if self.mode == 'train':\n","            sense_items, arg_items = senses[sent_idx], args[sent_idx]\n","            sense = []\n","            for idx in range(len(sense_items)):\n","                if sense_items[idx] != '_':\n","                    if len(sense) == 0:\n","                        sense.append(self.sense2idx[sense_items[idx]])\n","            sense_seq.append(sense)\n","\n","    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","    token_type_ids = [[0 if idx > 0 else 1 for idx in input_id]for input_id in input_ids]\n","    \n","    data_inputs = torch.tensor(input_ids)\n","    data_orig_tok_to_maps = torch.tensor(orig_tok_to_maps)\n","    data_lus = torch.tensor(lu_seq)\n","    data_token_type_ids = torch.tensor(token_type_ids)\n","    data_masks = torch.tensor(attention_masks)\n","    \n","    if self.mode == 'train':\n","        data_senses = torch.tensor(sense_seq)\n","        data_args = torch.tensor(arg_ids)\n","        bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_lus, data_senses, data_args, data_token_type_ids, data_masks)\n","    else:\n","        bert_inputs = TensorDataset(data_inputs, data_orig_tok_to_maps, data_lus, data_token_type_ids, data_masks)\n","    return bert_inputs"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"convert_to_bert_input_JointShallowSemanticParsing() missing 1 required positional argument: 'input_data'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconvert_to_bert_input_JointShallowSemanticParsing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: convert_to_bert_input_JointShallowSemanticParsing() missing 1 required positional argument: 'input_data'"]}],"source":["convert_to_bert_input_JointShallowSemanticParsing(trn)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
